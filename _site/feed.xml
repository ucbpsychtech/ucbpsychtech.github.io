<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-08T18:34:35-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Berkeley Psychology Graduate Blog</title><subtitle>Berkeley Psychology Graduate Blog</subtitle><entry><title type="html">The Thrill of Doing Science is That We Don’t Know What the Future Will Bring</title><link href="http://localhost:4000/behavior%20and%20systems%20neuroscience/cognitive%20neuroscience/memory/2019/08/07/Alice-Berners-Lee/" rel="alternate" type="text/html" title="The Thrill of Doing Science is That We Don't Know What the Future Will Bring" /><published>2019-08-07T00:00:00-07:00</published><updated>2019-08-07T00:00:00-07:00</updated><id>http://localhost:4000/behavior%20and%20systems%20neuroscience/cognitive%20neuroscience/memory/2019/08/07/Alice-Berners-Lee</id><content type="html" xml:base="http://localhost:4000/behavior%20and%20systems%20neuroscience/cognitive%20neuroscience/memory/2019/08/07/Alice-Berners-Lee/">&lt;p&gt;Alice Berners-Lee grew up in Lexington, Massachusetts and attended New York University where she majored in Neural Science and minored in Creative Writing. Alice started her PhD in 2014 at Johns Hopkins University in Baltimore under the supervision of Prof. David Foster. In 2017 the Foster lab was recruited to relocate to the University of California, Berkeley.&lt;/p&gt;

&lt;figure class=&quot;figure  figure--center&quot;&gt;
  &lt;img class=&quot;image&quot; src=&quot;/images/Blog_Alice.png&quot; alt=&quot;Alice Berners-Lee is a fifth year grad student working under the supervision of Prof. David Foster&quot; width=&quot;&quot; height=&quot;&quot; /&gt;
  &lt;figcaption class=&quot;caption&quot;&gt;Alice Berners-Lee is a fifth year grad student working under the supervision of Prof. David Foster&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;What have you learned about how the brain represents spatial information?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When an animal runs around, neurons in a particular area of the brain called the hippocampus tell the animal where it is in space. This means that as experimentalists, if we record from enough of these neurons, we could close our eyes and know pretty well where the animal is, just from the brain’s activity. Because we’re deciphering the brain’s code—the meaning of the electrical pulses that neurons use to communicate with each other—we call this procedure “decoding.” One of the most exciting parts of my research is that scientists have been studying this particular neural code—how the hippocampus represents space—for decades, so we understand it remarkably well. That foundation, combined with the particularities of hippocampal anatomy which allow us to record from many neurons simultaneously, means we can decode the brain’s activity with very high accuracy, to the point that you could call it a kind of mind-reading. When the animal is not running around but is standing still (often grooming or eating), these neurons fire seemingly all at once. What we see when we zoom in on these bursts of activity still amazes me: even though the animal is stationary, the hippocampus is actually playing out trajectories through space, at twenty times the speed at which the animal actually moved through the space. This phenomenon, called “replay,” is what I’ve been studying for the last four years of my PhD. 
Replay was first discovered during sleep and was initially posited to support long-term memory and learning. Subsequent studies found that replays also occur when the animal is awake, and that the trajectories depicted by these replay events tend to extend from the animal’s current position to the location of a reward, making them a candidate mechanism for “online” planning as well. Additionally, when there is a change in the environmental layout of rewards, replays reflect those changes, implicating them in learning where rewards are. 
But in order for replays to inform the behavior of the animal, they will have to be heard and processed by other areas in the brain such as areas that control an animal’s choices or movement. In learning what aspects of replay matter to downstream areas that receive that information, we also learn more about which aspects matter for the function of replay. I think the question of how replays in the hippocampus engage the rest of the brain is so interesting and important that I decided to focus on it for my PhD. If we can understand how hippocampal replay interacts with the whole brain, and how that changes across experience and learning, we can better understand the mechanisms and function of mental exploration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How will your work have real-world implications?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the most promising applications for the knowledge we uncover about replay is in mental illnesses such as schizophrenia. In mice genetically engineered to have something like schizophrenia, their hippocampal neurons behave abnormally. The neurons “burst” (fire off a handful of electrical pulses in quick succession) more than usual, but their bursts of activity don’t contain ordered replay—instead, the activity appears chaotic and uninterpretable. This deficit in replay is accompanied with deficits in social interaction, locomotor activity and working memory. Because there is still so much to be understood about replay in the typical brain state, we are far from understanding when and how replays get corrupted in disease and dysfunction. However, because replays have been shown to be important for memory and implicated in decision-making, investigating the mechanisms and functions of replay could help us understand more complex aspects of disease states.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally, what advances would you like to see in your field in the next two decades?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the most exciting advances in hippocampal replay is being able to detect these sequences online and be able to causally manipulate their trajectories. There are a lot of exciting hypotheses about why our brains replay information during sleep and awake rest, and causal manipulations are the gold standard to test these hypotheses. For example, when a rat learns to navigate to a hidden reward in a large environment, its replays tend to take trajectories towards that remembered goal location. This implies that replays may be guiding the rat’s subsequent behavior, but we can’t be sure. In order to test the effect of these replays on behavior, we need to decode the neurons’ activity as the animal performs the task, allowing us to interrupt the replays that would be headed to the goal. Then, if the rat isn’t able to navigate to that goal location, we would have more confidence that replays were guiding the rat’s behavior. Another advance I hope to see is to use newly invented recording equipment to investigate whether replay occurs in brain regions outside of the hippocampus. Until recently, technical difficulties in recording from enough neurons simultaneously prevented those experiments. Of course, the thrill of doing science is that we don’t know what the future will bring. It’s exciting to be in such a young field with so many emerging tools and unanswered questions.&lt;/p&gt;</content><author><name></name></author><summary type="html">Alice Berners-Lee grew up in Lexington, Massachusetts and attended New York University where she majored in Neural Science and minored in Creative Writing. Alice started her PhD in 2014 at Johns Hopkins University in Baltimore under the supervision of Prof. David Foster. In 2017 the Foster lab was recruited to relocate to the University of California, Berkeley.</summary></entry><entry><title type="html">Leveraging Big Data to Understand Emotion</title><link href="http://localhost:4000/social-personality/cognitive%20neuroscience/emotion%20recognition/artificial%20intelligence/2019/07/13/Alan-Cowen/" rel="alternate" type="text/html" title="Leveraging Big Data to Understand Emotion" /><published>2019-07-13T00:00:00-07:00</published><updated>2019-07-13T00:00:00-07:00</updated><id>http://localhost:4000/social-personality/cognitive%20neuroscience/emotion%20recognition/artificial%20intelligence/2019/07/13/Alan-Cowen</id><content type="html" xml:base="http://localhost:4000/social-personality/cognitive%20neuroscience/emotion%20recognition/artificial%20intelligence/2019/07/13/Alan-Cowen/">&lt;p&gt;Alan Cowen grew up in Chicago and attended Yale University where he double majored in Cognitive Science
and Applied Math. He is starting his fifth year in the psychology graduate program at UC Berkeley where he studies 
cognitive neuroscience and social-personality. He is supervised by Prof. Dacher Keltner.&lt;/p&gt;

&lt;figure class=&quot;figure  figure--center&quot;&gt;
  &lt;img class=&quot;image&quot; src=&quot;/images/Blog_Alan.jpg&quot; alt=&quot;Alan Cowen is a fifth year grad student working under the supervision of Prof. Dacher Keltner&quot; width=&quot;&quot; height=&quot;&quot; /&gt;
  &lt;figcaption class=&quot;caption&quot;&gt;Alan Cowen is a fifth year grad student working under the supervision of Prof. Dacher Keltner&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;What drew you to study emotion?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’m interested in emotions because I believe they underlie everything we do. If we didn’t have feelings like restlessness, anxiety, or excitement, we wouldn’t get up in the morning. We’d have no basis upon which to evaluate our lives, we could only be apathetic. Emotions motivate every thought we have. Even when solving math problems, it is curiosity, the drive to succeed, or the threat of failure, that motivate us to move from one cognitive step to another.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the goal of your research?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;My goal has been to come up with a taxonomy of emotion, which means moving beyond reductive models like the six basic emotions and the affective circumplex. I’ve developed a new framework for taxonomies of emotion, with accompanying statistical tools, and used it to derive taxonomies of vocal expression, facial expression, the emotions evoked by music and video, and more.&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;button&quot; href=&quot;https://s3-us-west-1.amazonaws.com/vocs/map.html&quot;&gt;Vocal Maps&lt;/a&gt;
 &lt;a class=&quot;button&quot; href=&quot;(https://s3-us-west-1.amazonaws.com/face28/map.html&quot;&gt;Facial Maps&lt;/a&gt;
 &lt;a class=&quot;button&quot; href=&quot;https://s3.amazonaws.com/musicemo/map.htm&quot;&gt;Music Maps&lt;/a&gt;
 &lt;a class=&quot;button&quot; href=&quot;https://s3.amazonaws.com/musicemo/map.htm&quot;&gt;Video Maps&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How will your findings have real-world implications?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’ve helped big tech companies give people the tools to recognize and appropriately respond to their own and others’ emotions. Smartphone cameras now use information about people’s emotional expressions to optimize pictures, choosing the best frame out of a 300 or so millisecond window of time after you release the shutter in part based on whether it captures the right emotions in a given context. Mental illness researchers are using taxonomies of emotion to develop diagnostic tests of emotion recognition. Autism researchers are beginning to successfully use augmented reality devices to help patients attend to others’ emotions, which requires understanding what those emotions actually are. In theory, I think we could help people save their marriages, fix bad habits, and improve their people skills by addressing these problems at their emotional roots.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally, what advances would you like to see in your field in the next two decades?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’m hoping that we can develop tools to accurately quantify people’s emotional behaviors in real time from video, audio, and language, in a context-sensitive way. Once we can do that, we can enter a world where computer models help us solve a vast array of human-centric problems. We could use algorithms that learn from millions of responses to buildings and cityscapes to evaluate or optimize architectural plans, for example, designing libraries that evoke awe, hospitals that make people feel safe, and so on. We could develop individualized assessments of mental illnesses. We could develop self-guided camera drones that take perfect wedding photos. There are a lot of possibilities. One of the ones that inspires me the most is the possibility to design machines that help vulnerable people who have difficulty recognizing or addressing their own emotional needs, infants, the elderly, people who struggle with severe mental illness, and so on, doing things as simple as changing the song or the channel when they aren’t enjoying themselves. That’s the kind of technology that could have helped my grandmother, who had severe Alzheimer’s, find enjoyable ways to spend her time even when people couldn’t be there to care for her.&lt;/p&gt;</content><author><name></name></author><summary type="html">Alan Cowen grew up in Chicago and attended Yale University where he double majored in Cognitive Science and Applied Math. He is starting his fifth year in the psychology graduate program at UC Berkeley where he studies cognitive neuroscience and social-personality. He is supervised by Prof. Dacher Keltner.</summary></entry><entry><title type="html">AI has much to Learn From Childhood Curiosity</title><link href="http://localhost:4000/developmental%20psychology/artificial%20intellignece/2019/07/13/Eliza-Kosoy/" rel="alternate" type="text/html" title="AI has much to Learn From Childhood Curiosity" /><published>2019-07-13T00:00:00-07:00</published><updated>2019-07-13T00:00:00-07:00</updated><id>http://localhost:4000/developmental%20psychology/artificial%20intellignece/2019/07/13/Eliza-Kosoy</id><content type="html" xml:base="http://localhost:4000/developmental%20psychology/artificial%20intellignece/2019/07/13/Eliza-Kosoy/">&lt;p&gt;Eliza Kosoy grew up in New York City and attended Emmanuel College where she studied Mathematics. She is starting her second year
in the psychology graduate program at UC Berkeley where she studies developmental psychology. She is supervised by Prof. Alison Gopnik.&lt;/p&gt;

&lt;figure class=&quot;figure  figure--center&quot;&gt;
  &lt;img class=&quot;image&quot; src=&quot;/images/Blog_Eliza.png&quot; alt=&quot;Eliza Kosoy is a second year grad student working under the supervision of Prof. Alison Gopnik&quot; width=&quot;&quot; height=&quot;&quot; /&gt;
  &lt;figcaption class=&quot;caption&quot;&gt;Eliza Kosoy is a second year grad student working under the supervision of Prof. Alison Gopnik&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;What drew you to study the intersection between child development and artificial intelligence (AI)?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I have always loved working with children, I find it extremely fun and rewarding, kids have wonderful personalities and are extremely funny and insightful to me. It is nice to take a break from adults every now and then. I am also drawn to the interesting nature of AI research. I studied math for my undergraduate studies, and then spent some time in both a computational cognitive science lab and child development lab at MIT after undergrad which led me to where I am today.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How can children inform artifical intelligence algorithms?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;My work attempts to understand how children learn novel concepts quickly and with little data in effort to make more human-like machine learning algorithms. Recently I have been working with Deepak Pathak, Pulkit Agrawal and Aloysha Efros at BAIR (Berkeley Artificial Intelligence Research) on how to make AI more curious. We are investigating intrinsic curiosity in children and how that might be beneficial in achieving goals. I am also working on making an Omniglot JR. dataset that people can use for machine learning tasks which captures how children create and write novel letters by analyzing their stroke patterns. We also look at how well kids are at classifying novel letters in the Omniglot dataset. This is work I started at MIT working with Josh Tenenbaum, Brenden Lake and Laura Schulz.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How will your findings have real-world implications?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AI has various applications for the world, ideally making certain mundane tasks more automated and thus freeing humans from certain kinds of work, perhaps we were not meant to be doing. Investigating the nature of curiously has potential for changing its purpose for people and maybe even encouraging everyone to be a bit more naturally curious.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally, what advances would you like to see in your field in the next two decades?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I would love to see an application in education where we could provide children all over the world with a low-cost personal teacher that helps them learn things faster based on their learning style and interests in the form of a tangible AI.&lt;/p&gt;</content><author><name></name></author><summary type="html">Eliza Kosoy grew up in New York City and attended Emmanuel College where she studied Mathematics. She is starting her second year in the psychology graduate program at UC Berkeley where she studies developmental psychology. She is supervised by Prof. Alison Gopnik.</summary></entry><entry><title type="html">Context is Crucial in Identifying Emotions</title><link href="http://localhost:4000/cognitive%20neuroscience/artificial%20intelligence/emotion%20recognition/2019/07/13/Mandy-Chen/" rel="alternate" type="text/html" title="Context is Crucial in Identifying Emotions" /><published>2019-07-13T00:00:00-07:00</published><updated>2019-07-13T00:00:00-07:00</updated><id>http://localhost:4000/cognitive%20neuroscience/artificial%20intelligence/emotion%20recognition/2019/07/13/Mandy-Chen</id><content type="html" xml:base="http://localhost:4000/cognitive%20neuroscience/artificial%20intelligence/emotion%20recognition/2019/07/13/Mandy-Chen/">&lt;p&gt;Mandy Chen grew up in the city Zhongshan in southern China and attended Peking University in Beijing where she studied Psychology. 
She is starting her fourth year in the psychology graduate program at UC Berkeley where she studies cognitive neuroscience. She 
is supervised by Prof. David Whitney.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What drew you to study emotion perception?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I have always had an interest in using quantitative methods to study mechanisms of the human mind. I was drawn to emotion perception because it is crucial for daily social interactions and it is a fascinating topic with a lot of gaps to fill. For example, one major advance in human object recognition is that we have taught computer vision models to recognize objects with amazing accuracy. However, recognizing emotion is a way more complex and challenging problem, and there have not been many successful models. I am hoping to bring some of the rigorous experimental techniques that I have learned in vision sciences (e.g. psychophysics) to help tackle this social science topic.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What has your research uncovered about facial expressions?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When it comes to reading a person’s state of mind, is it enough to just look at facial expressions? It is intuitive to say yes, and this has been the main direction of research for decades. However, my studies show that visual context – as in background and action – is both sufficient and necessary to accurately and rapidly recognize emotions. We blurred the faces and bodies of characters in muted video clips. Despite the characters’ invisible appearance, hundreds of participants were able to accurately read their emotions by extracting information from the visual context. We further show that the context provides a substantial and unique contribution beyond the information provided by the face and body. My research reveals that emotion recognition is, at its heart, an issue of context as much as faces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How will your findings have real-world implications?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Currently, companies are developing machine learning models to recognize emotions, but they only train their models on cropped faces and these models can only read emotions from faces. My research shows that only looking at faces does not reveal emotions very accurately and models should consider the context as well. The method that I developed could be used to quantify the contribution of facial expression versus visual context in any video of any scenario. My findings can lead us to understand under what scenarios visual context is more important and what mechanism the brain employs to perform the inference.&lt;/p&gt;

&lt;p&gt;In addition, current measures of emotional intelligence typically rely on decontextualized, oversimplified face stimuli. My findings suggest that tests of emotional intelligence will need to be revised to incorporate the separate but important issue of context. A person may be able to recognize static photos of facial emotions but fail to understand the displayed emotion accurately, unless they successfully incorporate the context. My method could eventually be used to evaluate how people with disorders like autism and schizophrenia recognize emotions in real time and help with their diagnoses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally, what advances would you like to see in your field in the next two decades?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;High-level cognitive functions such as emotion inference have been considered too difficult to solve in the field of computer vision, and very few artificial intelligence algorithms have succeeded in imitating them. I would like to see advances in the mechanisms underlying these high-level cognitive functions and hopefully build AI that can approach human abilities.&lt;/p&gt;</content><author><name></name></author><summary type="html">Mandy Chen grew up in the city Zhongshan in southern China and attended Peking University in Beijing where she studied Psychology. She is starting her fourth year in the psychology graduate program at UC Berkeley where she studies cognitive neuroscience. She is supervised by Prof. David Whitney.</summary></entry><entry><title type="html">Structural Forces at Play in Guiding Child Development</title><link href="http://localhost:4000/developmental%20psychology/cognitive%20neuroscience/childhood%20inequality/language/2019/07/13/Monica-Ellwood-Lowe/" rel="alternate" type="text/html" title="Structural Forces at Play in Guiding Child Development" /><published>2019-07-13T00:00:00-07:00</published><updated>2019-07-13T00:00:00-07:00</updated><id>http://localhost:4000/developmental%20psychology/cognitive%20neuroscience/childhood%20inequality/language/2019/07/13/Monica-Ellwood-Lowe</id><content type="html" xml:base="http://localhost:4000/developmental%20psychology/cognitive%20neuroscience/childhood%20inequality/language/2019/07/13/Monica-Ellwood-Lowe/">&lt;p&gt;Monica Ellwood-Lowe grew up in Milwaukee, Wisconsin and attended Stanford, where she majored in Psychology.
She is starting her third year in the psychology graduate program at UC Berkeley where she studies 
developmental psychology. She is supervised by Profs. Mahesh Srinivasan and Silva Bunge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What drew you to developmental research?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The honest answer is some combination of interest and random chance. I stumbled into Professor Anne Fernald’s lab after taking an undergraduate seminar with her, and spent some of my most formative research years learning about language development from her and Dr. Virginia Marchman. Once you develop the kind of depth of knowledge in a topic that they helped me cultivate, it tends to follow you around. But what keeps me in developmental research is walking into kindergartens in Oakland and seeing kids’ bright eyes. It feels like a time when you can really make a difference in someone’s life.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some of your work thus far has explored how childhood environment (with a particular focus on SES) shapes important facets of cognitive development as well as brain development. What are some of your key findings?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It’s so tempting to purport to study the environment by simply splitting kids up according to whose parents are highly educated with well-paying jobs (high-SES) and whose parents are less so (low-SES), and measure differences between them, particularly in the brain. Inevitably any difference you find will get translated and reported as a deficit for the low-SES kids. There are endless problems with this: What exactly is SES? What are the specific environmental mechanisms acting here? And, intriguingly for me, what are the benefits and trade-offs of different trajectories of brain development?&lt;/p&gt;

&lt;p&gt;In some of my work, I’ve found that the volume of the hippocampus, a stress-sensitive region of the brain that is central to learning and memory, varies in size across adolescence in a different way for girls whose families make more or less money. For girls whose parents are wealthier, the hippocampus takes the trajectory of development that we are used to seeing in the literature, with a peak around age 18, followed by a plateau. But for girls whose parents are less wealthy, a different pattern emerges, with peak hippocampal volume seemingly appearing later on. Even more interestingly, this finding is independent of their mothers’ hippocampal size, suggesting that it is not mediated by genetic or even shared environmental influences. Of course, this alone is unsatisfying. What does it mean for learning? What is the cause?&lt;/p&gt;

&lt;p&gt;In ongoing work with Professors Silvia Bunge and Mahesh Srinivasan, I’m specifically measuring aspects of the home environment that we think might be important, like the language surrounding a child during a typical day, and looking at how these relate to children’s attention, learning, and memory. I’m interested not in characterizing one set pattern of “optimal” development, but rather characterizing how development unfolds given the environmental context a child happens to grow up in. What is adaptive for one child given their context may be totally maladaptive for another. We can’t blindly assume the white upper-middle class context and developmental trajectory is optimal.&lt;/p&gt;

&lt;figure class=&quot;figure  figure--center&quot;&gt;
  &lt;img class=&quot;image&quot; src=&quot;/images/Blog_Monica.png&quot; alt=&quot;Hippocampal volume in teenage girls is modulated by household income&quot; width=&quot;&quot; height=&quot;&quot; /&gt;
  &lt;figcaption class=&quot;caption&quot;&gt;Hippocampal volume in teenage girls is modulated by household income&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How do you think your findings have the potential to inform public policy, education or early-life programs?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For one thing, if we take at face value the idea that children develop skills that best suit the demands of their environment—something which makes sense from an evolutionary perspective—it becomes very clear that schools typically start by building on the skills cultivated in higher-SES environments. Part of this is because there is less empirical research documenting the skills cultivated in lower-SES environments, a gap I’m hoping to fill with my research. And of course, part of this is because of the way schools have been purposefully structured in the first place.&lt;/p&gt;

&lt;p&gt;At the same time, there is no question that being in an environment where you are systematically deprived of resources and opportunity for advancement has negative consequences. For example, in research that we are writing up right now, Ruthe Foushee (another PhD student in the department), Professor Mahesh Srinivasan, and I have found preliminary evidence that financial constraints actually cause parents to talk less with their children. Even within a single family, we’ve found that parents talk less at the end of the month—when families are typically the most financially strapped—compared to the rest of the month. What is striking about this is that people tend to think of parenting as a skill that is stable within individuals, but simple fluctuations in external constraints seem to make a significant difference.&lt;/p&gt;

&lt;p&gt;I see this contributing to education and public policy in two ways: The first is that we need to stop thinking of development as having a clear pattern and instead think of it as an accumulating series of cognitive and biological tradeoffs. The second is that if we want lower-SES families to behave like higher-SES families, we need to be prepared to give them the external resources—financial and otherwise—that higher-SES families have.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally, what advances would you like to see in your field in the next two decades?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’d like to see a more nuanced understanding of how variation in the environment leads to variation in the brain and behavior of children—with all its pros and cons—and how external constraints affect not only children but their parents and the ways their parents parent. Our focus is so often on individuals, but I’d like us to pay more attention to larger, structural forces at play.&lt;/p&gt;

&lt;p&gt;I would also like to see an integration of research on SES and development and research on race and discrimination. Many people who study SES totally shy away from issues of race, which is bizarre when we consider the enormous racial wealth gap, and the fact that much of our present-day research on SES differences is actually rooted in historical research on supposed racial differences. In my view this is partly because the field of psychology, and cognitive neuroscience in particular, hasn’t figured out how to reconcile with its racist history; because biological determinism is still embedded in scientists’ collective beliefs even if their politics tell them not to say so out loud; and because there is a lack of shared understanding of the ways race has been constructed socially as opposed to biologically. It’s a tricky thing to study but I hope we can work together to try.&lt;/p&gt;</content><author><name></name></author><summary type="html">Monica Ellwood-Lowe grew up in Milwaukee, Wisconsin and attended Stanford, where she majored in Psychology. She is starting her third year in the psychology graduate program at UC Berkeley where she studies developmental psychology. She is supervised by Profs. Mahesh Srinivasan and Silva Bunge.</summary></entry></feed>